#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Jun 18 21:28:06 2019

@author: xiayezi
"""
import sys
sys.path.append("..")

from utils.conf import *
from torch.distributions.categorical import Categorical
from torch.distributions.one_hot_categorical import OneHotCategorical
from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical

# ======== Test for data generation =========
#data_batch = np.random.randint(0, 10**ATTRI_SIZE-1, (BATCH_SIZE, 1))
#data_candidates = np.random.randint(0, 10**ATTRI_SIZE-1, (BATCH_SIZE, SEL_CANDID))


def cat_softmax(probs, mode, tau=1, hard=False, dim=-1):
    if mode == 'REINFORCE' or mode == 'SCST':
        cat_distr = OneHotCategorical(probs=probs)       
        return cat_distr.sample(), cat_distr.entropy()
    elif mode == 'GUMBEL':
        cat_distr = RelaxedOneHotCategorical(tau, probs=probs)
        y_soft = cat_distr.rsample()
    
    if hard:
        # Straight through.
        index = y_soft.max(dim, keepdim=True)[1]
        y_hard = torch.zeros_like(probs, device=DEVICE).scatter_(dim, index, 1.0)
        ret = y_hard - y_soft.detach() + y_soft
    else:
        # Reparametrization trick.
        ret = y_soft
    return ret, ret

def weight_init(m):
    if isinstance(m, nn.Parameter):
        torch.nn.init.xavier_normal(m.weight.data)

class DataEncoderMLP(nn.Module):
    '''
        Input data is a [N_B,1] tensor, each number has ATTRI_SIZE attributes,
        each attribute has NUM_SYSTEM values. Output shape [N_B, 1, hidden]
        Test:
            t = DataEncoderMLP()
            test = t.gen_embedding(train_batch)  
            test_forward = t.forward(train_batch)
    '''
    def __init__(self, hidden_size=HIDDEN_SIZE):
        super(DataEncoderMLP, self).__init__()
        self.hidden_size = hidden_size
        self.emb_size = ATTRI_SIZE*NUM_SYSTEM

        self.lin = nn.Sequential(
            nn.Linear(self.emb_size, hidden_size),  # We concatenate two vectors
            nn.ReLU()
        )    
        
    def gen_embedding(self, data_batch):
        '''
            Change [N_B,1] data to [NB,1,ATTRI_SIZE*NUM_SYSTEM] tensor
        '''
        batch_size = data_batch.shape[0]
        data_embeddings = np.zeros((batch_size, 1, self.emb_size))
        for b in range(batch_size):
            for i in range(ATTRI_SIZE):
                tmp = int(np.mod(data_batch[b] / (NUM_SYSTEM**i), NUM_SYSTEM))
                index = int(tmp + NUM_SYSTEM*i)
                data_embeddings[b, 0, index] = 1.                
        return torch.tensor(data_embeddings).float().to(DEVICE)
    
    def forward(self, data_batch):
        data_embeddings = self.gen_embedding(data_batch)               
        return self.lin(data_embeddings) 


class MsgGenLSTM(nn.Module):
    '''
        Give the hidden generated by DataEncoderMLP, [N_B, 1, hidden], use GRU
        to generate message, [N_B, Max_len, hidden], and the mask having same shape
        The input size is voc_size, initialized as all zeros.
        
        t = DataEncoderMLP()
        h0=t.forward(data_batch)
        msg_gen = MsgGenLSTM()
        msg, mask, log = msg_gen.forward(h0, h0)
    '''
    def __init__(self, voc_size = MSG_VOCSIZE, hidden_size=HIDDEN_SIZE):
        super(MsgGenLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = voc_size
        self.output_size = voc_size
        
        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)
        self.out = nn.Linear(self.hidden_size, self.output_size)
        
        self.init_input = nn.Parameter(torch.zeros(1, self.input_size, device=DEVICE))
        
    def forward(self, h_0, c_0):
        '''
            The size of h_0 and c_0 is [N_B, 1, hidden], we should firstly convert 
            them to [N_B, hidden]
        '''        
        batch_size = h_0.size(0)
        #decoder_input = self.init_input.expand(batch_size, -1)
        decoder_input = torch.zeros((batch_size,self.input_size)).to(DEVICE)
        decoder_hidden = h_0.squeeze(1)
        decoder_cell = c_0.squeeze(1)
        message = []
        entropy = torch.zeros((batch_size,)).to(DEVICE)
        digits = []
        
        log_probs = 0.        

        for _ in range(MSG_MAX_LEN):
            decoder_hidden, decoder_cell = \
                self.lstm(decoder_input, (decoder_hidden, decoder_cell))
            
            digit = self.out(decoder_hidden)
            digits.append(digit)
            probs = F.softmax(digit, dim=1)
            
            if self.training:
                predict, entropy = cat_softmax(probs, mode=MSG_MODE, tau=args.tau, hard=MSG_HARD, dim=1)
            else:
                predict = F.one_hot(torch.argmax(probs, dim=1),num_classes=self.output_size).float()

            log_probs += torch.log((probs * predict).sum(dim=1))
            #_mask = _mask * (1 - predict[:, -1])        # The last position is EOS
            
            message.append(predict)
            decoder_input = predict
        
        message = torch.stack(message)           # Shape [MSG_MAX_LEN, N_B, MSG_VOCSIZE+1]
        digits = torch.stack(digits)
        
        return message, log_probs, entropy, digits



class MsgDecoderLSTM(nn.Module):
    '''
        t = DataEncoderMLP()
        h0=t.forward(data_batch)
        msg_gen = MsgGenLSTM()
        msg, mask, log = msg_gen.forward(h0, h0)
        msg_de = MsgDecoderLSTM()
        last_hidden, last_hidden = msg_de.forward(msg, mask)
    '''
    def __init__(self, input_size = MSG_VOCSIZE, hidden_size=HIDDEN_SIZE):
        super(MsgDecoderLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size

        self.lstm = nn.LSTMCell(self.input_size, self.hidden_size)
        self.init_hidden = self.init_hidden_and_cell()
        self.init_cell = self.init_hidden_and_cell()        

    def forward(self, msg):
        max_len = msg.size(0)
        batch_size = msg.size(1)    # msg shape: [MSG_MAX_LEN, N_B, MSG_VOCSIZE+1]

        last_hidden = self.init_hidden.expand(batch_size, -1).contiguous()
        last_cell = self.init_cell.expand(batch_size, -1).contiguous()        

        for t in range(max_len):
            hidden, cell = self.lstm(msg[t], (last_hidden, last_cell))
            last_hidden = hidden
            last_cell = cell       
        
        return last_hidden, last_cell
        
    def init_hidden_and_cell(self):
        return torch.zeros(1, self.hidden_size, device=DEVICE)     


class PredictorMLP(nn.Module):
    '''
        For listener to decide which candidate is the target. We concatenate 
        hidden states generated by listener decoder and dataencoder, then pass
        an MLP and get a distribution.
        One input is [N_B, Hidden]-size hidden given by MsgDecoderLSTM, another
        input is [N_B, SEL_CANDID] target+distructors. We should first expand
        hidden to [N_B, SEL_CANDID, Hidden], and use DataEncoderMLP to convert
        target+distructors to embeddings. We then concatenate them, pass them
        to MLP and softmax. The output should be [SEL_CANDID, 1] probability.
        t = DataEncoderMLP()
        msg_gen = MsgGenLSTM()
        msg_de = MsgDecoderLSTM()
        pred_MLP = PredictorMLP()
        h0=t.forward(data_batch)
        msg, mask, log = msg_gen.forward(h0, h0)
        last_hidden, _ = msg_de.forward(msg, mask)   
        pred_prob = pred_MLP.forward(last_hidden, data_candidates)  
    '''
    def __init__(self, hidden_size=HIDDEN_SIZE, cand_size=SEL_CANDID):
        super(PredictorMLP, self).__init__()
        self.hidden_size = hidden_size
        self.cand_size = cand_size
        self.data_encoder = DataEncoderMLP()
                
        self.lin = nn.Sequential(
            nn.Linear(hidden_size, hidden_size)  # We concatenate two vectors
            #nn.ReLU(),
            #nn.Linear(hidden_size, hidden_size),  # We concatenate two vectors
        )
        
    def forward(self,lis_hidden, data_candidates):
        '''
            lis_hidden: [N_B, Hidden], comes from MsgDecoderLSTM
            data_candidates: [N_B, SEL_CANDID]
        '''
        emb_candidates = []
        for i_cand in range(self.cand_size):
            tmp_emb = self.data_encoder.forward(data_candidates[:,i_cand]).squeeze(1) # To shape [N_B, Hidden]
            emb_candidates.append(tmp_emb)
        emb_candidates = torch.stack(emb_candidates).transpose(0,1) # To shape [N_B, SEL_CANDID, Hidden]        
        
        emb_hidden = self.lin(lis_hidden).unsqueeze(-1)
        candi_dot_hid = torch.bmm(emb_candidates, emb_hidden).squeeze(-1)      
       
        pred_vector = candi_dot_hid     # Shape should be [N_B, SEL_CANDID]
        #pred_prob = F.softmax(pred_vector)
        #lg_pred_prob = F.log_softmax(pred_vector)        
        #return lg_pred_prob, pred_prob
        return pred_vector
        
        
class SpeakingAgent(nn.Module):
    def __init__(self, voc_size = MSG_VOCSIZE, hidden_size=HIDDEN_SIZE):
        super().__init__()
        self.voc_size = voc_size
        self.hidden_size = hidden_size

        self.encoder = DataEncoderMLP(self.hidden_size)
        self.msg_generator = MsgGenLSTM(self.voc_size, self.hidden_size)

    def forward(self, data_batch):
        data_embs = self.encoder.forward(data_batch)
        msg, log_prob, entropy, digits = self.msg_generator.forward(data_embs, data_embs)

        return msg, log_prob, entropy, digits

    def reset_params(self):
        self.apply(weight_init)

class ListeningAgent(nn.Module):
    def __init__(self, voc_size = MSG_VOCSIZE, hidden_size=HIDDEN_SIZE, cand_size=SEL_CANDID):
        super().__init__()
        self.voc_size = voc_size
        self.hidden_size = hidden_size
        self.cand_size = cand_size

        self.predictor = PredictorMLP(self.hidden_size, self.cand_size)
        self.msg_decoder = MsgDecoderLSTM(self.voc_size, self.hidden_size)
    
    def forward(self, data_candidates, msg):
        last_hidden, _ = self.msg_decoder.forward(msg)
        pred_vector = self.predictor.forward(last_hidden, data_candidates)
        return pred_vector

    def reset_params(self):
        self.apply(weight_init)

'''
speaker = SpeakingAgent()
listener = ListeningAgent()

msg, mask, log_prob = speaker.forward(data_batch)
pred_prob = listener.forward(data_candidates, msg, mask)


# ============ Test for the whole structure===============        
t = DataEncoderMLP()
msg_gen = MsgGenLSTM()
msg_de = MsgDecoderLSTM()
pred_MLP = PredictorMLP()
h0=t.forward(data_batch)
msg, mask, log = msg_gen.forward(h0, h0)
last_hidden, _ = msg_de.forward(msg, mask)   
pred_prob = pred_MLP.forward(last_hidden, data_candidates)          
'''

        
        



